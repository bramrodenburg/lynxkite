## At a glance

[.text-center]
image::images/lynxkite-overview.png[LynxKite overview]

Like a typical web application, the backend runs in a datacenter (in a single Java VM) and serves
a rich frontend running in the userâ€™s browser. The server only serves static files (HTML,
JavaScript, CSS and images) and dynamic JSON responses. Avoiding dynamically generated HTML
eliminates a large class of security issues.

The following sections go into more detail about the architecture of both the backend and the
frontend.

## The Backend

The two technologies underpinning the LynxKite backend are http://spark.apache.org[Apache Spark],
a distributed computation framework, and https://www.playframework.com[Play! Framework], a web
application framework.

[.text-center]
image::images/lynxkite-backend.png[The backend]

Apache Spark can scale to thousands of machines. The distributed computation is coordinated by the
Spark Driver application, which is the LynxKite server in our case. This server is the only
proprietary component in the system. Besides acting as the Spark Driver, it also acts as the Play!
HTTP Server, serving static files and dynamic JSON content to the frontend.

### LynxKite installation modes

There are two ways for scheduling/hosting the Spark executors.

- In `local` mode, there is only one worker which runs as part of the main LynxKite process, this is
  ideal for single machine installations.

- In `yarn` mode we use the built-in support of Apache Spark for running as a Hadoop YARN
  application. In this case the Spark driver runs outside of YARN, starts a simple application
  master on YARN and asks it to start the executors.

### Storage

Apache Spark includes the Hadoop client libraries. This gives LynxKite the capability to access
file systems supported by Hadoop, such as HDFS, S3, and GCS. Data files can be imported from
and exported to these file systems. JDBC is used to import from and export to SQL databases.
Note that license issues prevent us from including the necessary Java library file in the
distribution: you should obtain this file (`mysql.mysql-connector-java-5.1.34.jar`) and copy
it into the directory specified by the `EXTRA_JARS` (As explained in <<kiterc-file>>.)

LynxKite needs some storage space on a filesystem to store internal graph data. This filesystem has
to be accessible by all workers. Because of this requirement, in most cases this needs to be a
distributed file system with fast access from the Spark workers. Ideal setups include using S3 when
running on Amazon EMR or using the HDFS of the same cluster when running using Hadoop YARN. On
single machine installations one case simply use local hard disk for this purpose.

Notably the LynxKite process runs on a single machine. This is a design decision on the part of
Apache Spark. While it introduces a single point of failure, it does not create a scaling
bottleneck, since all the computation work is performed by the Spark Workers.

## The Frontend

The LynxKite frontend is an https://angularjs.org[AngularJS] application.

[.text-center]
image::images/lynxkite-frontend.png[LynxKite frontend]

AngularJS provides routing, REST request handling, and custom HTML element management that is used
to define the LynxKite frontend modules. The defining feature of AngularJS is two-way data binding,
which is the glue that ties the LynxKite frontend modules together.

The project management view is regular dynamic HTML.

The 2D visualization is a single large embedded SVG element that is dynamically constructed by the
responsible LynxKite frontend module.

The 3D visualization is managed by another module and is a WebGL canvas used via the
http://threejs.org[Three.js] general-purpose JavaScript 3D library.

Minor utility libraries are also used, such as TinyColor for color manipulation.

