## Running on Amazon Elastic MapReduce (EMR)

There is specialized support for running LynxKite on EMR. There is a script which takes care of basic cluster
management, deployment and configuration of LynxKite on the cluster and remote restarts. To use LynxKite on
EMR, follow these steps:

. Install LynxKite as explained in the <<installing-binaries>> section.
In what follows, `$INSTALL_DIR` will refer to the installation directory of LynxKite. You can start
for example with setting `$INSTALL_DIR=/path/to/kite/install/dir`.

. Export your AWS credentials as environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.

. Make a copy of `$INSTALL_DIR/tools/emr_spec_template` somewhere. Letâ€™s say the name of your copy is
`my_cluster_spec`.

. Edit `my_cluster_spec`. You will give a name to your cluster and provide some configuration.
The settings are documented inside the template file and in the <<emr-file>> section. This
information is also available in the template file as comments.

. Start your cluster by:
`$INSTALL_DIR/tools/emr.sh start my_cluster_spec`

. Start kite on the cluster:
`$INSTALL_DIR/tools/emr.sh kite my_cluster_spec`
By default, LynxKite is running behind a firewall to prevent unauthorized access.

. Open a connection to your LynxKite instance:
`$INSTALL_DIR/tools/emr.sh connect my_cluster_spec`
Now you can access your LynxKite instance by entering http://localhost:4044 in your
browser. If you want to access YARN and Spark diagnostic tools, or more LynxKite instances
at the same time, then you need to update your Proxy Settings according to the EMR
https://docs.aws.amazon.com/ElasticMapReduce/latest/ManagementGuide/emr-connect-master-node-proxy.html[Proxy Settings]
documentation. You can stop this script by hitting Ctrl+C and restart it any number of times without
affecting your cluster.

. Access files on s3 from LynxKite. You can access any s3 file accessible by the aws user that
was used to launch the cluster (determined by `AWS_ACCESS_KEY_ID`) using the `S3$` prefix. If you
want to enable access to s3 files not accessible by this user, then you will have to add your own
prefix definition to `/home/hadoop/prefix_definitions.txt`. You will need to add something like:
`OTHER_S3="s3n://OTHER_AWS_ACCESS_KEY_ID:OTHER_AWS_SECRET_ACCESS_KEY@bucket/"`

. You can suspend your work any time. Note that this will destroy the cluster's storage so the data
created by LynxKite needs to be backed up.

  .. Make a copy of the metadata:
`$INSTALL_DIR/tools/emr.sh metacopy my_cluster_spec`
This will create a small file in your home directory in `~/kite_meta_backups`. This will contain your
list of projects and the history of executed operations in the projects.

  .. Make the copy of the data to S3 (this only works if `S3_DATAREPO` in `my_cluster_spec` is configured
properly):
`$INSTALL_DIR/tools/emr.sh s3copy my_cluster_spec`

  .. Then terminate LynxKite:
`$INSTALL_DIR/tools/emr.sh terminate my_cluster_spec`

  .. And then resume by:
    ... `$INSTALL_DIR/tools/emr.sh start my_cluster_spec`
    ... `$INSTALL_DIR/tools/emr.sh metarestore my_cluster_spec`
    ... `$INSTALL_DIR/tools/emr.sh kite my_cluster_spec`
    ... As you can see, stored data in S3 will be automatically used, but the stored metadata needs
        to be taken from your local machine manually.

. Once you are all done, you can permanently destroy your cluster by:
`$INSTALL_DIR/tools/emr.sh terminate my_cluster_spec`
Note that this will delete everything except for the raw data backed up in s3. Projects and
configurations will be gone.
