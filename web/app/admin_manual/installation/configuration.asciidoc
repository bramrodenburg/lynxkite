## Configuring LynxKite

### The `.kiterc` file

You need to create a site configuration to be able to run LynxKite. If you don’t have a config file
created yet, running the runner script will warn you and point you to a template config file. Just
copy that template to your home directory with name `.kiterc` and edit the settings as necessary.
The settings are documented inside the template file and in the <kiterc-file> section. This
information is also available in the template file as comments.

[[configure-directories]]
### Configure directories

This is a best practices section on how to configure various directories properly. The default
configuration provided by Hadoop may be suboptimal.

- DataNode directories should be on a large drive. Set `dfs.data.dir` and `dfs.datanode.data.dir`
  on every DataNode accordingly.
- NameNode and DataNode directories are better to not exist or at least be empty before you start
  the HDFS service the first time / deploy a new node. Check `dfs.name.dir`,
  `dfs.namenode.name.dir`, `dfs.data.dir` and `dfs.datanode.data.dir`.
- Datanode directories have to be owned by `hdfs:hadoop`.
- YARN log dirs should be on a large drive. Check `yarn.nodemanager.log-dirs` and `hadoop.log.dir`
  in the NodeManager and `hadoop.log.dir` in the ResourceManager configuration.
- YARN log dir has to be owned by `yarn:hadoop`
- Set the `yarn.nodemanager.local-dirs` (YARN app cache and file cache) to be on the appropriate
  drive. It should be sufficiently large (typically at least 100GB).

### Running LynxKite

LynxKite can be started with its runner script (e.g., `run-kite-stable.sh`). You can use the
`start` / `stop` / `restart` commands if you want to run it as a daemon. Or, you can use the
interactive command which starts up the program tied to your current shell session, so its output
will appear in the shell and pressing Ctrl-C or closing the shell session will terminate the
program.

### User authentication

In the default configuration user authentication is turned off. To enable user authentication,
you have to enable HTTPS (see next section). To securely set up user authentication:

- Make sure LynxKite is not accessible externally.
- Start LynxKite without authentication. Go to http://<lynxkite_url>/#/users and add at
  least one new administrator user. (Administrator users can create more users later.)
- Enable HTTPS in the `.kiterc` file.
- Restart LynxKite with authentication. Confirm that user credentials are now required for login.
- You can now make LynxKite externally accessible.

### HTTPS setup

To enable HTTPS, a certificate is required. For testing purposes a self-signed certificate is
included (`conf/localhost.self-signed.cert`). Its password is “asdasd”. This certificate does
not provide proper protection. (Lacking a trusted CA, the self-signed certificate can easily
be spoofed.) Using it will trigger a certificate warning in the user’s browser. It is recommended
to install a real certificate in its place.

Please refer to the <<kiterc-https>> setup for more details.

### Backups

Once LynxKite is up and running, take a moment to think about backups. The solution is entirely
dependent on the site and purpose of the installation. But consider that LynxKite deals with the
following data:

- *Import/export.*
- *Graph data in `KITE_DATA_DIR`.* The vertices, edges and attributes of the graph. This data
  is stored on a distributed file system. This typically provides resilience against data loss.
- *Project metadata in `KITE_META_DIR`.* The operations that have been executed on the project
  and their parameters. This data is small, and contains everything required to regenerate the
  graph data, assuming that the imported files are still available. This data is critical, and
  backups are recommended.

### Extra libraries

You can provide extra jar files that will be added to the `CLASSPATH` of LynxKite server by
setting the `KITE_EXTRA_JARS` option in `.kiterc`. A typical use case is installing JDBC drivers.
Just add the JAR file of the driver to `KITE_EXTRA_JARS` and you should be good to go.

### Integration with Hive

To enable direct integration with Apache Hive running on the same Hadoop cluster you need to
tell Spark about your Hive configuration. The simplest way to do this is to create a symbolic link
from the `conf` directory under your Spark installation directory to your hive-site.xml file. E.g.,
depending on your Hadoop distribution, something like this would do:

```
ln -s /etc/hive/conf/hive-site.xml ~/spark-1.6.0/conf/
```
