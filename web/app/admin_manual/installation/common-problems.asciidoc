## Common problems

### Java version

You need Java 7 or higher on the system.

### Write permission

You need write permission to the directory configured as `KITE_DATA_DIR`. On HDFS a home directory
is also required for the user that starts LynxKite. Example commands to create these directories:

- `sudo -u hdfs hadoop fs -mkdir hdfs://$NAMENODE:8020/kite_data`
- `sudo -u hdfs hadoop fs -mkdir hdfs://$NAMENODE:8020/user/$USER`
- `sudo -u hdfs hadoop fs -chown $USER hdfs://$NAMENODE:8020/kite_data`
- `sudo -u hdfs hadoop fs -chown $USER hdfs://$NAMENODE:8020/user/$USER`

### Port collision

By default LynxKite runs on port 2200. If this is used by another process, move LynxKite to a
different port (by changing the `KITE_HTTP_PORT` setting in `.kiterc`).

### YARN memory limit

When trying to start LynxKite on YARN, you may hit the YARN per-application memory limit. LynxKite
will log an error message, such as:
```
Required executor memory (12288+384 MB) is above the max threshold (7003 MB) of this cluster!
```
In this case you either need to set a lower executor memory (`EXECUTOR_MEMORY`) or increase the
limit in YARN by changing the `yarn.nodemanager.resource.memory-mb` and
`yarn.scheduler.maximum-allocation-mb` settings.

### YARN node manager kills the executors

By default Spark does not allocate enough memory overhead on top of the heap size for the JVM
itself. This can result in the nodemanager killing off executors. You can confirm this happening
by looking at the nodemanager logs. The remedy is to set the below in
`$SPARK_HOME/conf/spark-defaults.conf`: `spark.yarn.executor.memoryOverhead 4000`

### Disk runs out of space

This can be due to many reasons such as various logs, Spark shuffle files, the LynxKite data files
etc. See <<configure-directories>> for more details and fixes.


