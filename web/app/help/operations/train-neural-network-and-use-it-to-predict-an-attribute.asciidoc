### Train a neural network and use it to predict an Attribute

Trains a neural network using the graph's vertex attributes and edges. Then uses
this trained neural network to make a prediction on the same graph.

Currently the computation is not distributed, so please do not use it on really
big graphs.
====
[[label]] Attribute to predict::
The partially defined attribute that you want to predict. The current
implementation only supports attributes between -1 and 1.

[[output]] Save as::
The prediction will be saved as an attribute created under this name.

[[features]] Predictors::
The attributes that will be used as the input of the prediction.

[[networkLayout]] Network layout:: The network consists in iterating one st
+
  - **LSTM or GRU:** The network works like an RNN: there is the same neural
  network at each vertex. Each vertex sends its output to its neighbors, then
  the vertices use the sum of them as inputs GRU and LSTM are two different
  layouts for this.
  http://colah.github.io/posts/2015-08-Understanding-LSTMs/[Further information
  about LSTM and GRU.]
  - **MLP:**

[[networkSize]] Size of the network::
The number of nodes in one layer of the neural network.

[[radius]] Iterations in prediction::
The number of times when the vertices send information to their neighbors.

[[hideState]] Hide own state::
If it is set to true vertices do not know their own label, but their neighbors
can still see it.

[[forgetFraction]] Forget fraction::
In every training iteration each vertex forgets its label with the probability
given here. Neither the vertices themselves, nor their neighbors see the forgotten
labels.

[[knownLabelWeight]] Weight for known labels::
If the forget fraction is greater than 1, then the errors from the non-forgetting
nodes are multiplied by this number. So if it is set to a small number, then the
errors from non-forgetting vertices count less than the ones from the forgetting
vertices.

[[numberOfTrainings]] Number of trainings::
The training is performed on randomly chosen subgraphs. In the first round each
node gets a small subgraph and performs a few iterations of training. After this
the average of the learned weights is calculated. In the second round each node
gets another small subgraph and performs a few iterations of training, starting
from the average weights. After this the average of the learned weights is
calculated again, and so on. You can set here the number of these turns.

[[iterationsInTraining]] Iterations in training::
The number of iterations in one round of training.

[[subgraphsInTraining]] Subgraphs in training::
The number of subgraphs chosen in one training round.

[[minTrainingVertices]] Minimum training subgraph size::
The minimum size of subgraphs chosen for training.

[[maxTrainingVertices]] Maximum training subgraph size::
The maximum size of subgraphs chosen for training.

[[trainingRadius]] Radius for training subgraphs::

[[seed]] Seed:: Random seed for initializing network weights and choosing
subgraphs.
====
