// To avoid repetition, add text here and include it in multiple places.
//
// Include syntax:
//   include::{g}[tag=my-tag]
//
// Note that there are no implicit newlines around the inclusion. This means you could
// (intentionally or not) include something as inline content.

tag::database-access[]
The database name is the JDBC connection string without the `jdbc:` prefix.
(For example `mysql://127.0.0.1/my_database?user=batman&password=alfred`.)
See <<jdbc-details>> for more details.
end::database-access[]

tag::database-import[]
[[db]] Database::
include::glossary.asciidoc[tag=database-access]

[[table]] Table or view::
Table to import from. This can be anything that can be put into the `SELECT * FROM table`
statement.

[[columns]] Columns::
The comma-separated list of columns to import.

[[key]] Key column::
This numerical column is used to partition the SQL query. The range from `min(key)` to `max(key)`
will be split into a sub-range for each Spark worker, so they can each query a part of the data in
parallel.
end::database-import[]

tag::database-export[]
[[db]] Database::
The database to write to.
+
include::{g}[tag=database-access]

[[table]] Table::
The table to create in the database.

[[attrs]] Attributes::
Attributes to export.

[[delete]] Overwrite table if it exists::
If disabled, the operation will fail if the target table already existis.
If enabled, all data in the table will be deleted before inserting the new data. Be careful!
end::database-export[]

tag::file-export[]
There are two ways to access the exported data:

 - Set _Destination path_ to `<auto>`. A download link will be created as a project scalar.
   You can download the file through this link. This works even for large files as long as
   they fit on your hard drive.
 - Set _Destination path_ to a distributed file system path that you can access. This allows
   you to access the exported data using the standard tools for the file system. A download link
   will still be provided, but the standard tools can allow for higher throughput access.
end::file-export[]

tag::csv-import[]
[[files]] Files::
Upload a file using the button on the right, or specify the path to an existing file set.
Wildcard (`+foo/*.csv+`) and glob (`+foo/{bar,baz}.csv+`) patterns are accepted.
+
The file must be specified as a <<prefixed-paths, prefixed path>>, such as `+MY_ROOT$dir/*.csv+`.

[[header]] Header::
The list of column names. This parameter serves two purposes:
+
 - Each column specified will be imported as an attribute.
 - If this line is found in the CSV, it will be ignored.
+
When left as `<read first line>`, the header is read automatically from the file.

[[delimiter]] Delimiter::
The column delimiter in the CSV. Typical values are `,` (comma), `;` (semicolon), `\t` (tab).

[[omitted]] (optional) Comma separated list of columns to omit::
If you don't want to read all columns of your CSV, list here the name of columns you don't need.

[[filter]] (optional) Filtering expression::
A JavaScript expression can be provided here to restrict the import to lines that match a
condition. For example `id != ''` would ignore lines where the `id` column is empty.

[[allow_corrupt_lines]] Tolerate ill-formed lines::
When set to `no`, the import process will stop with an error as soon as it encounters
a line that does not match the header. When set to `yes`, erroneous lines will be
silently skipped.
end::csv-import[]

tag::table-import[]
[[table]] Table::
The table to import from.
end::table-import[]

tag::import-box[]
[[imported_columns]] Columns to import::
The columns to import. Leave empty to import all columns.

[[limit]] Limit::
Number of rows to import at the most. Leave empty to import all rows.

[[sql]] SQL::
SQL query to apply to the data before importing it. Filtering before importing can be much more
efficient than filtering after importing.

[[imported_table]] Table GUID::
Click this button to actually kick off the import. You can click it again later to repeat the
import. (Useful if the source data has changed.)
end::import-box[]

tag::random-seed[]
=====
LynxKite operations are typically deterministic. If you re-run an operation with
the same random seed, you will get the same results as before. To get a truly independent random
re-run, make sure you choose a different random seed.

The default value for random seed parameters is randomly picked, so only very
rarely do you need to give random seeds any thought.
=====
end::random-seed[]

tag::random-attribute[]
Generates a new random Double attribute with the specified distribution, which can be either
(1) a Standard Normal (i.e., Gaussian) distribution with a mean of 0 and a standard deviation of 1, or (2)
a Standard Uniform distribution where values fall between 0 and 1.

====
[[name]] Attribute name::
The new attribute will be created under this name.

[[dist]] Distribution::
The desired random distribution.

[[seed]] Seed::
The random seed.
+
include::{g}[tag=random-seed]
====
end::random-attribute[]

tag::global-aggregators[]
The available aggregators are:

 * For `Double` attributes:
 ** `sum`
 ** `average`
 ** `min`
 ** `max`
 ** `count` (number of cases where the attribute is defined)
 ** `first` (arbitrarily picks a value)
 ** `std_deviation` (standard deviation)

 * For other attributes:
 ** `count` (number of cases where the attribute is defined)
 ** `first` (arbitrarily picks a value)
end::global-aggregators[]

tag::local-aggregators[]
The available aggregators are:

 * For `Double` attributes:
 ** `average`
 ** `count` (number of cases where the attribute is defined)
 ** `count_distinct` (the number of distinct values)
 ** `first` (arbitrarily picks a value)
 ** `max`
 ** 'median'
 ** `min`
 ** `most_common`
 ** `count_most_common` (the number of occurrences of the most common value)
 ** `set` (all the unique values, as a `Set` attribute)
 ** `std_deviation` (standard deviation)
 ** `sum`
 ** `vector` (all the values, as a `Vector` attribute)

 * For `String` attributes:
 ** `most_common`
 ** `count_most_common` (the number of occurrences of the most common value)
 ** `count_distinct` (the number of distinct values)
 ** `majority_50` (the value that 50% agree on, or empty string)
 ** `majority_100` (the value that 100% agree on, or empty string)
 ** `count` (number of cases where the attribute is defined)
 ** `vector` (all the values, as a `Vector` attribute)
 ** `set` (all the unique values, as a `Set` attribute)

 * For other attributes:
 ** `most_common`
 ** `count_most_common` (the number of occurrences of the most common value)
 ** `count_distinct` (the number of distinct values)
 ** `count` (number of cases where the attribute is defined)
 ** `set` (all the unique values, as a `Set` attribute)
end::local-aggregators[]

tag::sql-box[]

The following tables are available for SQL access for project inputs:

 - All the vertex attributes can be accessed in the `vertices` table.
+
Example: `select count(*) from {maybe-tick}{prefix}vertices{maybe-tick} where age < 30`

 - All the edge attributes can be accessed in the `edge_attributes` table.
+
Example: `select max(weight) from {maybe-tick}{prefix}edge_attributes{maybe-tick}`
+
You can not query the `edge_attributes` table if there are no edge attributes, even if the edges
themselves are defined.

 - All the scalars can be accessed in the `scalars` table.
+
Example: `select {backtick}!vertex_count{backtick} from {maybe-tick}{prefix}scalars{maybe-tick}`

 - All the edge and vertex attributes can be accessed in the `edges` table. Each row of this
table represents an edge. The attributes of the edge are prefixed with `edge_`, while the attributes
of the source and destination vertices are prefixed with `src_` and `dst_` respectively.
+
Example:
`select max(edge_weight) from {maybe-tick}{prefix}edges{maybe-tick} where src_age < dst_age`

 - The `belongs_to` table is defined for each segmentation of a project or a segmentation. It
contains the vertex attributes for the connected pairs of base and segmentation vertices prefixed
with `base_` and `segment_` respectively.
+
Examples:

 * `select count(*) from {backtick}{prefix}communities.belongs_to{backtick} group by segment_id`
 * `select base_name from {backtick}{prefix}communities.belongs_to{backtick} where segment_name =
 "COOKING"`

Backticks (`{backtick}`) are used for escaping table and column names with special characters.

You can browse the list of available tables and columns by clicking on the
+++<label class="btn btn-primary">Tables ‚èµ</label>+++
button.

====
[[sql]] SQL query::
The query. Press Ctrl-Enter to save your changes while staying in the editor.
====
end::sql-box[]
