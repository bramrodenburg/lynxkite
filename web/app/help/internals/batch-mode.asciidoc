[[batch-mode]]
## Batch processing API

LynxKite can be run in two modes. The first is a long-running server that offers an interactive
user interface. (This is documented in the rest of this guide.) The other is *batch mode* wherein
LynxKite executes a script and terminates when it is finished. This is useful for running periodic
tasks and for integrating with other systems.

The batch mode cannot be run at the same time as interactive mode. The two modes operate on the
same data. So a batch script could apply changes to a graph created in interactive mode, or
conversely interactive mode could be used to explore a graph created in batch mode, simply by
using the same project names.

Usage:

[subs=normal]
 ./run-kite.sh batch _script_file_ [_parameters_]

Where `_parameters_` is a list of `key:value` pairs.

### Script file syntax

Batch script files are written in the http://www.groovy-lang.org/[Groovy] programming language.

The script will receive the command line arguments in the `params` map that is injected into its
namespace.

The script can interact with LynxKite through the `lynx` object that is injected into its
namespace.

 - `GroovyProject **lynx.loadProject**(String name)` loads an existing project.
 - `GroovyProject **lynx.newProject**()` creates a new project.
 - `DataFrameReader **lynx.sqlContext**` provides access to Spark SQL.
   See the
   http://spark.apache.org/docs/latest/sql-programming-guide.html[Spark SQL and DataFrame Guide]
   to learn more. (The Java examples also apply to Groovy.)
 - `DataFrame **lynx.sql**(String query)` is a shortcut to `lynx.sqlContext.sql`. It executes a
   Spark SQL query and returns the result as an Apache Spark DataFrame.

The properties of `GroovyProject` are:

 - `Map<String, GroovyProject> **segmentations**` is the list of segmentations for the project.
 - `Map<String, GroovyScalar> **scalars**` is the list of scalars. `GroovyScalar` has a `toString()`
   and a `toDouble()` method, they return its value as a string/double respectively.
 - `Map<String, GroovyAttribute> **vertexAttributes**` and
   `Map<String, GroovyAttribute> **edgeAttributes**` are the vertex and edge attributes.
   `GroovyAttribute` has a `histogram(Int n)` method that
   returns a histogram of the attribute with `n` buckets.
 - `DataFrame **df**` gives you access to the vertex set and vertex attributes of the project as an
   Apache Spark DataFrame. See the
   http://spark.apache.org/docs/latest/sql-programming-guide.html[Spark SQL and DataFrame Guide]
   to learn about working with DataFrames.

`GroovyProject` has a method for every operation. These methods take named arguments for each
operation parameter. Rather than documenting these methods in detail, the recommended approach is
to run the operations in interactive mode, enter the <<project-history, history editor>> and the
<<saving-a-workflow, workflow editor>> within. The correct Groovy code can then be copied from the
workflow editor.

Additional methods of `GroovyProject` are:

 - Workflows can be run with the `**runWorkflow**()` method. Workflows are internally
   identified by their creation timestamps in addition to their names. The first argument of
   `runWorkflow` is the workflow name either with or without the timestamp. If the timestamp is
   omitted, the latest version of the workflow is used. The workflow parameters can be passed as
   named arguments to `runWorkflow`, same as other operations.
 - `**saveAs**(String name)` can be used to save a project into LynxKite. Until `saveAs` is called,
   the persistent state in LynxKite is not affected by the batch script.
 - `**copy**()` returns a copy of the `GroovyProject` object that can be manipulated independently
   from the original.

### Full example

In this example we create a script that reads edges from a CSV file, calculates PageRank, exports
PageRank to another CSV, and prints the number of vertices and the time the whole script took.

----
start_time = System.currentTimeMillis()
// Import input, calculate PageRank, export output.
project = lynx.newProject()
project.importVerticesAndEdgesFromSingleCSVFileset(
  files: params['input'],
  header: '<read first line>',
  delimiter: ',',
  src: 'src',
  dst: 'dst',
  filter: '',
  allow_corrupt_lines: 'no',
  omitted: '')
project.pageRank(
  name: 'page_rank',
  weights: '!no weight',
  iterations: '5',
  damping: '0.85')
project.exportVertexAttributesToFile(
  path: params['output'],
  link: 'exported_csv',
  attrs: 'stringID,page_rank',
  format: 'CSV')

// Print metrics.
count = project.scalars['vertex_count']
time = (System.currentTimeMillis() - start_time) / 1000
println "$count vertices processed in $time seconds."

// Example of Spark SQL integration.
project.df.printSchema()
project.df.groupBy('gender').count().show()
project.df.registerTempTable('example')
df = lynx.sql('select * from example where gender = "Male"')
df.show()
df.write().format('com.databricks.spark.csv').option('header', 'true').save('males.csv')
df.write().format('parquet').save('males.parquet')
df.write().format('json').save('males.json')

// Save the project into LynxKite so it can be explored via the web interface.
project.saveAs('Batch PageRank')
----

When running the script we must use <<prefixed-paths>> for the file names. For example:

 ./run-kite.sh batch pagerank.groovy input:UPLOAD$/data-2015.csv output:UPLOAD$/pagerank-2015.csv

See the `kitescripts` directory in the LynxKite installation for more complex example scripts.
