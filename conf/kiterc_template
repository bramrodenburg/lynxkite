# Set to the directory containing your spark installation
export SPARK_HOME=$HOME/spark-${SPARK_VERSION}

# Specifies what cluster we use. Options are:
#  local - for single machine installs
#  yarn-client - for using yarn (hadoop v2). Please also set YARN_* settings below.
#  spark://master-name:master-port - for standalone spark clusters
export SPARK_MASTER=local

# The directory where Kite stores metadata about projects. Must be on the local file system.
# Do not forget to set up backups!
export KITE_META_DIR=$HOME/kite_meta

# The data where graph data is stored. Should be on a distributed fs (hdfs, s3, etc) unless
# this is a single machine install.
# Some examples: 
#
# KITE_DATA_DIR= s3n://key:secret@data/
# In this scenario, the users can access files in directory s3n://data. To access a file
# such as s3n://data/exports, users should say DATA$/exports. They will not have access
# to the credentials (i.e., key and secret in this example). If either the password or
# the key changes, KITE_DATA_DIR (and only KITE_DATA_DIR) should be updated accordingly.
#
# KITE_DATA_DIR=s3n://key:secret@
# Here, the users have access to the root directory of the s3n filesystem. However, they
# still do not have access to the credentials. Now they can refer to the exports directory
# like this: DATA$/data/exports (assuming the same directory structure as in the previous
# example.
#
# KITE_DATA_DIR=hdfs://nameservice1:8020/user/kite/data/
# Credentials play no part here, but if the data directory is moved someplace else
# (say, hdfs://nameservice1:8020/user/kite/data2), only the KITE_DATA_DIR variable
# has to be updated. As usual, users can refer to data in their directory using
# the DATA$ notation: e.g., DATA$/uploads/*

# KITE_DATA_DIR=file:$HOME/kite_data/
# This is a single machine configuration. As you see, the full uri name must be used here. 
# (e.g., file:$HOME/kite_data/ rather than just $HOME/kite_data/).


export KITE_DATA_DIR=file:$HOME/kite_data/

# In the above examples, the variable KITE_DATA_DIR was used to define the DATA$
# prefix. Should other prefixes be necessary (e.g., an empty prefix can be
# defined for the users to have unlimited access to everything), they should
# be specified in a separete file. See the file root_definitions.txt for
# details.
# export KITE_ADDITIONAL_ROOT_DEFINITIONS=<location of your root definitions file>

# The PID file for Kite servers.
export KITE_PID_FILE=$HOME/kite.pid

# The user registry file for Kite servers.
export KITE_USERS_FILE=$HOME/kite_users

# Needed if you want to run against yarn.
# The directory with the YARN configuration.
# export YARN_CONF_DIR=/etc/hadoop/...
# Number of executors.
# export YARN_NUM_EXECUTORS=2

# Specify how much memory is available for Kite on a single worker machine.
# Ignored for single machine installs, see SPARK_DRIVER_MEMORY_MB for that case.
export EXECUTOR_MEMORY=1g

# Number of cores per executor Kite should use. (For a YARN deployment, use this instead of
# the YARN_CORES_PER_EXECUTOR option that was available in pre-1.2 versions.)
export NUM_CORES_PER_EXECUTOR=4

# Specify how much memory is available for Kite on the master machine in megabytes.
# For standalone single machine instances this also determines executor memory and
# EXECUTOR_MEMORY is ignored in that case.
export KITE_MASTER_MEMORY_MB=1024

# Port for the Kite HTTP server to listen on. Must be >=1000.
export KITE_HTTP_PORT=9000

# HTTP port for the watchdog. If this is set, the startup script will start a watchdog as well
# which will automatically restart the Kite server if it detects any problem.
# export KITE_WATCHDOG_PORT=9999

# A local path that exists on all workers and the master and will be used for storing
# temporary spark/hadoop files. This can get big, so if you have a small root filesystem
# and an extra large drive mounted somewhere then you need to point this to somewhere on
# the large drive. On the other hand performance of this drive has big effect on overal speed,
# so SSD is a nice option here.
export KITE_LOCAL_TMP=/tmp

# A colon (:) delimited list of JAR files that should be loaded on Kite CLASSPATH. (Will be loaded
# both on master and workers.)
# Wildcards are not supported.
# Typical use case is to configure additional JDBC drivers, all you need to do is to add the
# jar file here.
export KITE_EXTRA_JARS=

# Options needed if you want to use authentication and https.

# Just uncomment the lines between the ==== lines for simple, fake certificate setup.
# ===========================================================
# export KITE_APPLICATION_SECRET=${KITE_RANDOM_SECRET}
#
# Port for the Kite HTTPS server to listen on. Must be >=1000.
# export KITE_HTTPS_PORT=9001
#
# Keystore file and passwd with the HTTPS keys. Just leave as is for fake HTTPS certificate.
# For a real HTTPS setup, see https://github.com/biggraph/biggraph/issues/1178
# export KITE_HTTPS_KEYSTORE=${KITE_DEPLOYMENT_CONFIG_DIR}/localhost.self-signed.cert
# export KITE_HTTPS_KEYSTORE_PWD=asdasd
# ===========================================================

# Only needed for Google Auth ask the R&D team for further instructions if you think you need this.
# export KITE_GOOGLE_CLIENT_SECRET='???'
