# Set to the directory containing your spark installation
export SPARK_HOME=$HOME/spark-${SPARK_VERSION}

# Specifies what cluster we use. Options are:
#  local - for single machine installs
#  yarn-client - for using yarn (hadoop v2). Please also set YARN_* settings below.
#  spark://master-name:master-port - for standalone spark clusters
export SPARK_MASTER=local

# The directory where Kite stores metadata about projects. Must be on the local file system.
# Do not forget to set up backups!
export KITE_META_DIR=$HOME/kite_meta

# The data where graph data is stored. Should be on a distributed fs (hdfs, s3, etc) unless
# this is a single machine install. Please note that giving the full uri (e.g., file:/...)
# is now mandatory.
export KITE_DATA_DIR=file:$HOME/kite_data/

# Configuration file to specify what file paths users are allowed to access
# using what prefixes. Consider moving prefix_definitions.txt out of
# KITE_DEPLOYMENT_CONFIG_DIR and change KITE_PREFIX_DEFINITIONS accordingly. 
export KITE_PREFIX_DEFINITIONS=${KITE_DEPLOYMENT_CONFIG_DIR}/prefix_definitions.txt

# The PID file for Kite servers.
export KITE_PID_FILE=$HOME/kite.pid

# The user registry file for Kite servers.
export KITE_USERS_FILE=$HOME/kite_users

# Needed if you want to run against yarn.
# The directory with the YARN configuration.
# export YARN_CONF_DIR=/etc/hadoop/...
# Number of executors.
# export YARN_NUM_EXECUTORS=2

# Specify how much memory is available for Kite on a single worker machine.
# Ignored for single machine installs, see KITE_MASTER_MEMORY_MB for that case.
export EXECUTOR_MEMORY=1g

# Number of cores per executor Kite should use. (For a YARN deployment, use this instead of
# the YARN_CORES_PER_EXECUTOR option that was available in pre-1.2 versions.)
export NUM_CORES_PER_EXECUTOR=4

# Specify how much memory is available for Kite on the master machine in megabytes.
# For single machine installs this also determines executor memory and
# EXECUTOR_MEMORY is ignored in that case.
export KITE_MASTER_MEMORY_MB=1024

# Port for the Kite HTTP server to listen on. Must be >=1000.
export KITE_HTTP_PORT=9000

# HTTP port for the watchdog. If this is set, the startup script will start a watchdog as well
# which will automatically restart the Kite server if it detects any problem.
# export KITE_WATCHDOG_PORT=9999

# A local path that exists on all workers and the master and will be used for storing
# temporary spark/hadoop files. This can get big, so if you have a small root filesystem
# and an extra large drive mounted somewhere then you need to point this to somewhere on
# the large drive. On the other hand performance of this drive has big effect on overal speed,
# so SSD is a nice option here.
export KITE_LOCAL_TMP=/tmp

# A colon (:) delimited list of JAR files that should be loaded on Kite CLASSPATH. (Will be loaded
# on the master and distributed to the workers.)
# Wildcards are not supported.
# Filenames have to be absolute paths.
# Typical use case is to configure additional JDBC drivers, all you need to do is to add the
# jar file here.
export KITE_EXTRA_JARS=

# Options needed if you want to use authentication and https.

# Just uncomment the lines between the ==== lines for simple, fake certificate setup.
# ===========================================================
# export KITE_APPLICATION_SECRET=${KITE_RANDOM_SECRET}
#
# Port for the Kite HTTPS server to listen on. Must be >=1000.
# export KITE_HTTPS_PORT=9001
#
# Keystore file and passwd with the HTTPS keys. Just leave as is for fake HTTPS certificate.
# For a real HTTPS setup, see https://github.com/biggraph/biggraph/issues/1178
# export KITE_HTTPS_KEYSTORE=${KITE_DEPLOYMENT_CONFIG_DIR}/localhost.self-signed.cert
# export KITE_HTTPS_KEYSTORE_PWD=asdasd
# ===========================================================

# Only needed for Google Auth ask the R&D team for further instructions if you think you need this.
# export KITE_GOOGLE_CLIENT_SECRET='???'
